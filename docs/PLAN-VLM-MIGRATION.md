# VectorGov ‚Äî Plano de Migra√ß√£o: SpanParser ‚Üí Qwen3-VL + Reconciliator

**Vers√£o:** 2.1 (Fase 0 conclu√≠da)
**Data:** 2026-02-06
**Autor:** Abimael / Claude
**Prop√≥sito:** Documento de planejamento para implementa√ß√£o no Claude Code (IDE)

---

## 1. Contexto e Motiva√ß√£o

### 1.1 Problema Original

O pipeline atual usa Docling + SpanParser (regex) para extrair estrutura hier√°rquica de documentos legais brasileiros. Esse pipeline sofre de:

- **Docling gera texto inconsistente**: line breaks arbitr√°rios, espa√ßamento n√£o-determin√≠stico
- **Regex √© fr√°gil**: o SpanParser falha quando cita√ß√µes legais aparecem no texto (ex: "conforme Art. 23, ¬ß 2¬∫" dispara falso-positivo de detec√ß√£o de artigo)
- **ADDRESS_MISMATCH**: chunks gerados com `span_id` incorreto por falha no regex ‚Äî problema que motivou a cria√ß√£o de toda a camada de seguran√ßa PR10/PR12/PR13

### 1.2 Solu√ß√£o: Paradigma Visual

Substituir extra√ß√£o baseada em texto (Docling ‚Üí regex) por extra√ß√£o visual (Qwen3-VL) + texto determin√≠stico (PyMuPDF):

```
ANTES:
  PDF ‚Üí Docling (texto sujo) ‚Üí SpanParser (regex) ‚Üí canonical_builder ‚Üí Milvus

DEPOIS:
  PDF ‚Üí PyMuPDF (texto determin√≠stico) + Qwen3-VL (estrutura visual) ‚Üí Reconciliator ‚Üí Milvus
```

O VLM "v√™" a p√°gina como um humano ‚Äî identifica visualmente onde come√ßa um artigo, par√°grafo, inciso, sem depender de regex sobre texto corrompido.

### 1.3 Infraestrutura

- **RunPod A40 48GB**: Qwen3-VL-8B-Instruct (~17GB FP16) + BGE-M3 (~2GB) = ~19GB, ~29GB livres para batch
- **VPS (vectorgov.io)**: FastAPI, Milvus, Neo4j, PostgreSQL, Redis, MinIO
- **BYOL**: Cliente traz seu pr√≥prio LLM para query/response ‚Äî RunPod dedicado apenas √† ingest√£o

---

## 2. Arquitetura de Seguran√ßa Existente (PRs)

### 2.1 Vis√£o Geral

O pipeline atual possui uma cadeia de m√≥dulos de seguran√ßa numerados (PR10, PR12, PR13) que garantem integridade das evidence links ‚Äî o diferencial do VectorGov.

**Princ√≠pio fundamental**: Nenhum chunk chega no Milvus sem garantia de que o evidence link vai funcionar.

### 2.2 M√≥dulos e Seus Pap√©is

#### PR10 ‚Äî `snippet_extractor.py` (Query-time)

- **Fun√ß√£o**: Localiza trecho exato no texto can√¥nico e extrai snippet com janela de contexto (¬±240 chars)
- **Mecanismo**: Se `canonical_hash` confere, usa slicing puro `canonical_text[start:end]`; sen√£o fallback via `find()`
- **Regra**: Best-effort ‚Äî se n√£o encontrar, retorna `None` sem quebrar a evid√™ncia
- **Camada**: Opera em query-time, n√£o em ingest√£o

#### PR12 ‚Äî `canonical_builder.py` (Ingest√£o ‚Äî Constru√ß√£o)

- **Fun√ß√£o**: Constr√≥i texto can√¥nico markdown enquanto rastreia offsets de cada dispositivo legal
- **Mecanismo**: `normalize_canonical_text()` garante determinismo (NFC, LF, trailing whitespace); `compute_canonical_hash()` gera SHA256 anti-mismatch
- **Output**: `CanonicalResult` com `canonical_text`, `offsets` dict, e `canonical_hash`

#### PR13 ‚Äî `pr13_validator.py` + `canonical_offsets.py` (Ingest√£o ‚Äî Gate)

- **Fun√ß√£o**: Gate cr√≠tico pr√©-Milvus ‚Äî valida o "trio Evidence"
- **Regras obrigat√≥rias**:
  1. `canonical_start >= 0`
  2. `canonical_end >= canonical_start`
  3. `canonical_hash != "" e != None`
- **Comportamento**: Qualquer viola√ß√£o √© CRITICAL ‚Üí aborta documento inteiro (zero rows no Milvus) ‚Üí cria alarme no PostgreSQL
- **Princ√≠pio**: "Validate ALL before inserting ANY"

#### `canonical_offsets.py` (Resolu√ß√£o de Filhos)

- **Fun√ß√£o**: Resolve offsets de filhos (¬ß, incisos, al√≠neas) dentro do range do pai
- **Mecanismo**: `resolve_child_offsets()` busca texto deterministicamente ‚Äî exatamente 1 ocorr√™ncia dentro do range do pai
- **Erros**: `OffsetResolutionError` com atributos `document_id`, `span_id`, `device_type`, `reason` (NOT_FOUND, AMBIGUOUS, EMPTY_TEXT)

#### `canonical_validation.py` (Hardening de Insert)

- **Fun√ß√£o**: Valida formato do `node_id` antes do insert no Milvus
- **Regras**: Prefixo correto (`leis:` / `acordaos:`), sem sufixo `@P`, `logical_node_id` preenchido
- **Modos**: `validate_and_fix` (auto-corrige) e `validate_only` (rejeita)

#### `alarm_service.py` (Observabilidade)

- **Fun√ß√£o**: Persiste alarmes no PostgreSQL com deduplica√ß√£o
- **Features**: Stats agregadas, filtros, bulk resolve, bloqueio de evid√™ncias de documentos comprometidos
- **M√©todo cr√≠tico**: `has_critical_alarms_for_document()` ‚Äî pode bloquear exibi√ß√£o de evidence links

### 2.3 Fluxo Integrado Atual

```
PDF ‚Üí Docling ‚Üí SpanParser
                    ‚Üì
            canonical_builder (PR12: offsets na ingest√£o)
                    ‚Üì
            canonical_offsets (resolve filhos dentro do pai)
                    ‚Üì
            pr13_validator (gate: trio v√°lido ou aborta tudo)
                    ‚Üì
            canonical_validation (hardening node_id/prefix)
                    ‚Üì
                  Milvus ‚úÖ
                    ‚Üì
            snippet_extractor (PR10: slicing puro com hash anti-mismatch)
                    ‚Üì
              Evidence Link ao PDF no MinIO üéØ
```

---

## 3. Impacto da Migra√ß√£o nos M√≥dulos de Seguran√ßa

### 3.1 Matriz de Impacto

| M√≥dulo | Status | Justificativa |
|--------|--------|---------------|
| `pr13_validator.py` | ‚úÖ INTACTO | Gate agn√≥stico ‚Äî valida trio, n√£o sabe quem produziu os dados |
| `alarm_service.py` | ‚úÖ INTACTO | Pura observabilidade ‚Äî recebe severity/type/document_id, persiste |
| `canonical_validation.py` | ‚úÖ INTACTO | Valida√ß√£o de formato node_id ‚Äî agn√≥stico ao parser |
| `snippet_extractor.py` | ‚úÖ INTACTO | Opera em query-time ‚Äî n√£o sabe como offsets foram gerados |
| `canonical_builder.py` | üîÑ REDUZ A UTILS | Fun√ß√µes de normaliza√ß√£o/hash sobrevivem, constru√ß√£o do canonical morre |
| `canonical_offsets.py` | üîÑ TRANSFORMA | `resolve_child_offsets()` vira valida√ß√£o/fallback do Reconciliator |
| SpanParser | ‚ùå MORRE | Substitu√≠do pelo Qwen3-VL + Reconciliator |
| Docling | ‚ùå MORRE | Substitu√≠do pelo PyMuPDF |

### 3.2 Detalhamento das Transforma√ß√µes

#### `canonical_offsets.py` ‚Üí `canonical_utils.py` (‚úÖ FEITO na Fase 0)

**Extra√≠do para `src/utils/canonical_utils.py`** (Fase 0 conclu√≠da):
```python
normalize_canonical_text()   # NFC + LF + trailing whitespace + final \n
compute_canonical_hash()     # SHA256 determin√≠stico
validate_offsets_hash()      # Anti-mismatch check
```

**NOTA**: O plano original mencionava `canonical_builder.py` mas esse arquivo n√£o existe no codebase.
As fun√ß√µes estavam em `canonical_offsets.py`, que agora re-exporta de `canonical_utils.py` para backward compatibility.

**Morre** (substitu√≠do pelo Reconciliator nas fases posteriores):
```python
extract_offsets_from_parsed_doc()  # N√£o haver√° mais ParsedDocument
build_canonical_with_offsets()     # Reconciliator gera offsets via bbox
_format_node_id()                  # Reconciliator constr√≥i node_id do JSON do VLM
```

#### `canonical_offsets.py` ‚Äî Transforma√ß√£o

**Morre**:
```python
extract_offsets_from_parsed_doc()  # N√£o haver√° mais ParsedDocument
```

**Sobrevive como valida√ß√£o/fallback**:
```python
resolve_child_offsets()            # Double-check do bbox matching OU fallback
OffsetResolutionError              # Diagn√≥stico de falhas
resolve_offsets_recursive()        # Pode ser reutilizado se Reconciliator precisar
```

---

## 4. Novos Componentes

### 4.1 PyMuPDF Extractor

**Responsabilidade**: Extrair texto determin√≠stico + coordenadas de blocos do PDF.

**Output**:
```python
@dataclass
class PyMuPDFResult:
    canonical_text: str              # Texto completo, determin√≠stico
    canonical_hash: str              # SHA256 do texto normalizado
    pages: list[PageBlocks]          # Blocos por p√°gina com coordenadas

@dataclass
class TextBlock:
    text: str
    bbox: tuple[float, float, float, float]  # x0, y0, x1, y1
    page_number: int
    block_index: int
    char_start: int                  # Offset no canonical_text
    char_end: int                    # Offset no canonical_text
```

**Propriedade cr√≠tica**: O canonical_text √© DETERMIN√çSTICO ‚Äî mesmo PDF sempre gera mesmo texto, byte a byte. Isso torna o hash anti-mismatch muito mais confi√°vel do que o canonical constru√≠do pelo PR12.

**Localiza√ß√£o**: `src/extraction/pymupdf_extractor.py`

### 4.2 VLM Service (Qwen3-VL)

**Responsabilidade**: Receber imagem da p√°gina do PDF e retornar estrutura hier√°rquica com bounding boxes.

**Input**: Imagem da p√°gina (renderizada via PyMuPDF)
**Output**:
```python
@dataclass
class VLMElement:
    type: str                        # "article", "paragraph", "inciso", "alinea", "chapter"
    number: str                      # "1", "2", "I", "a", "√∫nico"
    text: str                        # Texto do dispositivo (OCR do VLM)
    bbox: tuple[float, float, float, float]  # x0, y0, x1, y1 (coordenadas na p√°gina)
    parent: Optional[str]            # Refer√™ncia ao pai (ex: "ART-001")
    page_number: int
    confidence: float                # Confian√ßa do VLM na classifica√ß√£o

@dataclass
class VLMPageResult:
    page_number: int
    elements: list[VLMElement]
    raw_json: dict                   # Resposta bruta do VLM para debug
```

**Deploy**: vLLM no RunPod A40 ‚Äî `vllm serve "Qwen/Qwen3-VL-8B-Instruct" --max_model_len 8096`

**Prompt de extra√ß√£o**: Deve solicitar JSON estruturado com type, number, bbox, parent. Instruct variant (sem `<think>` overhead).

**Localiza√ß√£o**: `src/extraction/vlm_service.py`

### 4.3 Reconciliator

**Responsabilidade**: M√≥dulo CENTRAL da migra√ß√£o. Ponte entre VLM (estrutura visual) e PyMuPDF (texto determin√≠stico).

**Fun√ß√µes**:
1. Mapear cada bbox do VLM para text blocks do PyMuPDF (matching por sobreposi√ß√£o de coordenadas)
2. Gerar `node_id` can√¥nico a partir do tipo/n√∫mero/pai retornado pelo VLM
3. Calcular `canonical_start` e `canonical_end` (offsets no canonical_text do PyMuPDF)
4. Construir rela√ß√£o `parent_id` ‚Üí `child_ids` para hierarquia
5. Validar coer√™ncia: o texto no offset corresponde ao que o VLM identificou?

**Interface**:
```python
@dataclass
class ReconciledChunk:
    node_id: str                     # "leis:LEI-14133-2021#ART-023"
    chunk_id: str                    # "LEI-14133-2021#ART-023"
    parent_id: Optional[str]         # "leis:LEI-14133-2021#CAP-V"
    span_id: str                     # "ART-023"
    text: str                        # Texto exato do PyMuPDF (n√£o do VLM OCR)
    device_type: str                 # "article", "paragraph", "inciso", "alinea"
    canonical_start: int             # Offset no canonical_text
    canonical_end: int               # Offset no canonical_text
    canonical_hash: str              # SHA256 do canonical_text
    page_number: int
    bbox: tuple[float, float, float, float]  # Coordenadas f√≠sicas no PDF
    confidence: float                # Confian√ßa herdada do VLM

class Reconciliator:
    def __init__(
        self,
        pymupdf_result: PyMuPDFResult,
        vlm_results: list[VLMPageResult],
        document_id: str,
    ): ...

    def reconcile(self) -> list[ReconciledChunk]: ...
    def reconcile_element(self, element: VLMElement) -> Optional[ReconciledChunk]: ...
```

**Estrat√©gia de matching bbox ‚Üí text blocks**:
```
1. Para cada VLMElement:
   a. Encontrar text blocks do PyMuPDF cuja bbox tem IoU > threshold com bbox do VLM
   b. Concatenar texto desses blocks ‚Üí texto do chunk
   c. Localizar esse texto no canonical_text ‚Üí char_start, char_end
   d. Se matching direto falha ‚Üí usar resolve_child_offsets() como fallback
   e. Se fallback tamb√©m falha ‚Üí OffsetResolutionError ‚Üí alarme
```

**Localiza√ß√£o**: `src/extraction/reconciliator.py`

### 4.4 Integrity Validator (P√≥s-Reconcilia√ß√£o)

**Responsabilidade**: Camada adicional de valida√ß√£o que opera ENTRE o Reconciliator e o PR13. Verifica invariantes que dependem da rela√ß√£o entre VLM output e PyMuPDF output.

**Valida√ß√µes**:
```python
class IntegrityValidator:
    def validate_reconciled_chunks(
        self,
        chunks: list[ReconciledChunk],
        canonical_text: str,
    ) -> IntegrityResult:
        """
        Valida√ß√µes:
        1. Slicing: canonical_text[start:end] come√ßa com primeiras palavras do chunk.text
        2. Hierarquia: todo filho tem start >= parent.start e end <= parent.end
        3. Sem sobreposi√ß√£o: chunks irm√£os n√£o se sobrep√µem
        4. Cobertura: artigo cobre todos seus filhos
        5. Ordena√ß√£o: offsets s√£o monotonicamente crescentes dentro de cada n√≠vel
        """
```

**Localiza√ß√£o**: `src/extraction/integrity_validator.py`

---

## 5. Novo Fluxo Integrado

```
PDF
 ‚îÇ
 ‚îú‚îÄ‚îÄ‚Üí PyMuPDF Extractor
 ‚îÇ      ‚îú‚îÄ‚îÄ canonical_text (determin√≠stico)
 ‚îÇ      ‚îú‚îÄ‚îÄ canonical_hash (SHA256 via canonical_utils)
 ‚îÇ      ‚îî‚îÄ‚îÄ text_blocks[] (texto + bbox + char_start/char_end por bloco)
 ‚îÇ
 ‚îî‚îÄ‚îÄ‚Üí Qwen3-VL Service (RunPod A40)
        ‚îî‚îÄ‚îÄ vlm_elements[] (type + number + bbox + parent + confidence por dispositivo)
                ‚îÇ
                ‚ñº
        ‚îå‚îÄ Reconciliator ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  1. bbox VLM ‚Üî text blocks PyMuPDF   ‚îÇ
        ‚îÇ  2. canonical_start/end por chunk     ‚îÇ
        ‚îÇ  3. node_id can√¥nico                  ‚îÇ
        ‚îÇ  4. parent-child hierarchy            ‚îÇ
        ‚îÇ  5. texto = PyMuPDF (n√£o VLM OCR)     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
        ‚îå‚îÄ IntegrityValidator ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Slicing, hierarquia, sobreposi√ß√£o   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
        ‚îå‚îÄ canonical_validation.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚Üê EXISTENTE, intacto
        ‚îÇ  node_id format (leis:/acordaos:)  ‚îÇ
        ‚îÇ  sem @P, logical_node_id filled    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
        ‚îå‚îÄ pr13_validator.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚Üê EXISTENTE, intacto
        ‚îÇ  trio Evidence gate                ‚îÇ
        ‚îÇ  abort doc se viola√ß√£o             ‚îÇ
        ‚îÇ  cria alarme via alarm_service     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
        ‚îå‚îÄ alarm_service.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚Üê EXISTENTE, intacto
        ‚îÇ  persiste no PostgreSQL            ‚îÇ
        ‚îÇ  bloqueia docs com CRITICAL alarms ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
        ‚îå‚îÄ BGE-M3 (RunPod A40) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  dense + sparse embeddings         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
                Milvus ‚úÖ  +  Neo4j (hierarquia)
                   ‚îÇ
              (query-time)
                   ‚îÇ
                   ‚ñº
        ‚îå‚îÄ snippet_extractor.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚Üê EXISTENTE, intacto
        ‚îÇ  slicing puro + hash anti-mismatch ‚îÇ
        ‚îÇ  fallback find() se necess√°rio     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
             Evidence Link üéØ
             (page_number + bbox ‚Üí highlight no PDF no MinIO)
```

---

## 6. Chunk Schema Final

```python
{
    # Identifica√ß√£o
    "node_id": "leis:LEI-14133-2021#ART-023-PAR-002",
    "chunk_id": "LEI-14133-2021#ART-023-PAR-002",
    "logical_node_id": "leis:LEI-14133-2021#ART-023-PAR-002",
    "document_id": "LEI-14133-2021",
    "span_id": "PAR-023-2",

    # Texto (fonte: PyMuPDF, N√ÉO VLM OCR)
    "text": "¬ß 2¬∫ O processo de contrata√ß√£o direta...",

    # Hierarquia
    "parent_id": "leis:LEI-14133-2021#ART-023",
    "device_type": "paragraph",
    "chunk_level": 3,  # 1=cap, 2=art, 3=¬ß/inciso

    # Evidence Trio (PR13)
    "canonical_start": 1847,
    "canonical_end": 2103,
    "canonical_hash": "a1b2c3d4...",  # SHA256 do canonical_text

    # Localiza√ß√£o f√≠sica no PDF (NOVO ‚Äî n√£o existia no pipeline anterior)
    "page_number": 12,
    "bbox": [72.0, 340.5, 520.3, 410.8],  # x0, y0, x1, y1

    # Embeddings (BGE-M3)
    "dense_vector": [...],
    "sparse_vector": {...},

    # Metadados
    "tipo_documento": "LEI",
    "numero": "14133",
    "ano": 2021,
    "confidence": 0.97,  # Confian√ßa do VLM na classifica√ß√£o
}
```

**Diferen√ßas vs. schema anterior**:
- `page_number` e `bbox`: NOVOS ‚Äî permitem highlight visual direto no PDF (MinIO), sem depender apenas de char offsets
- `confidence`: NOVO ‚Äî permite filtrar/alertar chunks com baixa confian√ßa do VLM
- `text`: fonte muda de "canonical_builder markdown" para "PyMuPDF raw text"

---

## 7. Fases de Implementa√ß√£o

### FASE 0 ‚Äî Prepara√ß√£o ‚úÖ CONCLU√çDA (2026-02-06)

**Objetivo**: Reorganizar c√≥digo existente sem quebrar nada.

**Status**: CONCLU√çDA. Todas as tarefas essenciais foram executadas. Testes: 294 passed, 34 skipped, 0 failures.

**Tarefas realizadas**:

1. ‚úÖ **Extrair `canonical_utils.py`** de `canonical_offsets.py` (N√ÉO de `canonical_builder.py` ‚Äî esse arquivo n√£o existe no codebase):
   - Criado `src/utils/canonical_utils.py` com `normalize_canonical_text()`, `compute_canonical_hash()`, `validate_offsets_hash()`
   - `canonical_offsets.py` agora importa e re-exporta de `canonical_utils.py` (backward compatibility)
   - Import usa try/except para suportar testes que carregam m√≥dulos diretamente (sem package)
   - Atualizado `src/utils/__init__.py` com novos exports
   - **Arquivos modificados**: `src/utils/canonical_utils.py` (NOVO), `src/utils/__init__.py`, `src/chunking/canonical_offsets.py`

2. ‚úÖ **Criar estrutura de diret√≥rios**:
   - Criado `src/extraction/__init__.py` com docstring descrevendo os m√≥dulos futuros
   ```
   src/
   ‚îú‚îÄ‚îÄ extraction/          # NOVO (placeholder para Fases 1-3)
   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
   ‚îú‚îÄ‚îÄ utils/
   ‚îÇ   ‚îú‚îÄ‚îÄ canonical_utils.py  # NOVO (extra√≠do de canonical_offsets)
   ‚îÇ   ‚îî‚îÄ‚îÄ normalization.py    # Existente
   ‚îú‚îÄ‚îÄ chunking/            # EXISTENTE (deprecar gradualmente)
   ‚îÇ   ‚îú‚îÄ‚îÄ canonical_offsets.py  # Agora re-exporta de canonical_utils
   ‚îÇ   ‚îî‚îÄ‚îÄ chunk_materializer.py
   ‚îú‚îÄ‚îÄ ingestion/           # EXISTENTE
   ‚îÇ   ‚îî‚îÄ‚îÄ models.py        # Atualizado com campos VLM
   ‚îî‚îÄ‚îÄ services/            # EXISTENTE
   ```

3. ‚úÖ **Atualizar `ProcessedChunk` model** em `ingestion/models.py`:
   - Adicionados campos: `page_number: int = Field(-1)`, `bbox: list[float] = Field(default_factory=list)`, `confidence: float = Field(0.0)`
   - Todos os campos existentes mantidos intactos
   - **Arquivo modificado**: `src/ingestion/models.py`

4. ‚úÖ **Atualizar schema Milvus `leis_v4`** (mant√©m nome `leis_v4`, N√ÉO renomear ‚Äî hardcoded em 12+ locais):
   - Adicionados ao `docs/leis_v4.json`: `page_number` (Int64, fieldID 125), `bbox` (VarChar max 256 para JSON, fieldID 126), `confidence` (Float, fieldID 127)
   - Adicionados ao dict de inser√ß√£o Milvus em `pipeline.py` (linhas ~1310-1316)
   - **IMPORTANTE**: A collection Milvus precisa ser recriada para incluir os novos campos. O JSON √© a refer√™ncia para recria√ß√£o.
   - **Arquivos modificados**: `docs/leis_v4.json`, `src/ingestion/pipeline.py`

5. ‚è≥ **Compatibilidade MinIO Evidence Storage** ‚Äî ADIADA para fases posteriores:
   - `integrity_validator.py` Layer 6 N√ÉO existe no codebase atual
   - `EvidenceResponse` em `evidence/models.py` N√ÉO existe no codebase atual
   - `docs/ANALISE_MINIO_EVIDENCE_STORAGE.md` N√ÉO existe no codebase atual
   - Esses componentes ser√£o criados nas Fases 3-4 quando o pipeline VLM estiver funcional

**Discrep√¢ncias encontradas entre plano e codebase**:
- `canonical_builder.py` n√£o existe ‚Äî fun√ß√µes estavam em `canonical_offsets.py`
- `pr13_validator.py` n√£o existe como m√≥dulo separado ‚Äî valida√ß√£o est√° em `pipeline.py:validate_chunk_invariants()`
- `snippet_extractor.py` n√£o existe ‚Äî √© `extract_snippet_by_offsets()` dentro de `canonical_offsets.py`
- `alarm_service.py` n√£o existe no codebase atual
- `evidence/models.py` e `EvidenceResponse` n√£o existem no codebase atual

**Testes executados**:
- `test_pr13_canonical_offsets.py`: 27 passed ‚úÖ
- `test_pr13_offset_resolution.py`: todos passed ‚úÖ
- `test_pr13_span_parser_offsets.py`: todos passed ‚úÖ
- `test_pr13_materializer_integration.py`: todos passed ‚úÖ
- `test_ingestion_pipeline.py`: 17 passed, 2 skipped ‚úÖ
- Suite completa: **294 passed, 34 skipped, 0 failures** ‚úÖ
- Erros de cole√ß√£o pr√©-existentes (n√£o causados pela Fase 0): `test_article_validator.py` (fastapi ausente), `test_chunk_materializer_split.py` (import direto), `test_rel_type_classification.py` (prometheus registry)

**Crit√©rio de conclus√£o**: ‚úÖ Todos os testes existentes passam, nova estrutura de diret√≥rios existe, model atualizado com campos VLM, schema Milvus documentado com novos campos.

---

### FASE 1 ‚Äî PyMuPDF Extractor (2-3 dias)

**Objetivo**: Substituir Docling como fonte de texto. Ter canonical_text determin√≠stico.

**Tarefas**:
1. **Implementar `pymupdf_extractor.py`**:
   ```python
   class PyMuPDFExtractor:
       def extract(self, pdf_path: str) -> PyMuPDFResult:
           """
           1. Abrir PDF com fitz (PyMuPDF)
           2. Para cada p√°gina:
              a. Extrair text blocks com get_text("dict") ‚Üí blocos com bbox
              b. Extrair texto corrido com get_text("text") ‚Üí para canonical_text
           3. Construir canonical_text concatenando texto de todas as p√°ginas
           4. Calcular char_start/char_end de cada bloco no canonical_text
           5. Normalizar via normalize_canonical_text()
           6. Computar canonical_hash via compute_canonical_hash()
           """

       def render_page_image(self, pdf_path: str, page_num: int, dpi: int = 300) -> bytes:
           """Renderiza p√°gina como imagem para enviar ao VLM."""
   ```

2. **Testes**:
   - Determinismo: extrair mesmo PDF 3x ‚Üí mesmo canonical_text e hash
   - Offsets: char_start/char_end de cada bloco apontam pro texto correto via slicing
   - Cobertura: todos os blocos de texto est√£o representados no canonical_text
   - Edge cases: PDFs com m√∫ltiplas colunas, tabelas, headers/footers

3. **Teste de compara√ß√£o**: Extrair mesmo documento com Docling e PyMuPDF, comparar qualidade do texto

4. **Definir formato do canonical.md**:
   - O canonical.md ser√° texto puro extra√≠do pelo PyMuPDF (n√£o markdown constru√≠do pelo canonical_builder)
   - Confirmar que `normalize_canonical_text()` (em `canonical_utils.py`) funciona com texto PyMuPDF ‚Äî a fun√ß√£o j√° √© agn√≥stica ao formato (strip + lower)

5. **Teste de determinismo do canonical.md**:
   - Mesmo PDF extra√≠do 3x pelo PyMuPDF ‚Üí mesmo texto ‚Üí mesmo SHA-256 hash
   - Texto extra√≠do deve ser est√°vel entre vers√µes do PyMuPDF (fixar vers√£o no requirements)

**Crit√©rio de conclus√£o**: PyMuPDF extrai texto determin√≠stico de PDFs legais brasileiros com offsets corretos por bloco. Hash √© est√°vel. Formato do canonical.md definido como texto puro PyMuPDF.

**Depend√™ncias**: `pip install PyMuPDF` (j√° dispon√≠vel no ambiente)

---

### FASE 2 ‚Äî VLM Service (3-5 dias)

**Objetivo**: Ter o Qwen3-VL rodando no RunPod e retornando hierarquia estruturada com bbox.

**Tarefas**:
1. **Deploy do Qwen3-VL no RunPod**:
   ```bash
   vllm serve "Qwen/Qwen3-VL-8B-Instruct" --max_model_len 8096 --gpu-memory-utilization 0.35
   ```
   - Configurar endpoint HTTP
   - Testar health check e throughput

2. **Implementar `vlm_service.py`**:
   ```python
   class VLMService:
       def __init__(self, endpoint_url: str): ...

       def extract_structure(self, page_image: bytes, page_number: int) -> VLMPageResult:
           """
           1. Enviar imagem da p√°gina para Qwen3-VL via API (vLLM OpenAI-compatible)
           2. Prompt: solicitar JSON com dispositivos legais, seus tipos, n√∫meros, bbox, hierarquia
           3. Parsear resposta JSON
           4. Validar: cada elemento tem type, number, bbox v√°lidos
           5. Retornar VLMPageResult
           """

       def extract_document(self, page_images: list[bytes]) -> list[VLMPageResult]:
           """Extrai estrutura de todas as p√°ginas (batch processing)."""
   ```

3. **Engenharia de Prompt** ‚Äî Desenvolver e testar prompt que retorne:
   ```json
   {
     "elements": [
       {
         "type": "article",
         "number": "23",
         "text": "Art. 23. O processo de contrata√ß√£o direta...",
         "bbox": [72.0, 120.5, 520.3, 180.8],
         "parent": null,
         "confidence": 0.98
       },
       {
         "type": "paragraph",
         "number": "1",
         "text": "¬ß 1¬∫ A contrata√ß√£o direta ser√° processada...",
         "bbox": [72.0, 185.0, 520.3, 240.2],
         "parent": "ART-023",
         "confidence": 0.96
       }
     ]
   }
   ```

4. **Testes**:
   - Accuracy: VLM identifica corretamente artigos, ¬ß¬ß, incisos, al√≠neas de trechos da Lei 14.133
   - Bbox precision: bbox retornado cobre o texto visualmente (sem cortar, sem excesso)
   - Hierarquia: parent correto (inciso do ¬ß aponta pro ¬ß, n√£o direto pro artigo)
   - Edge cases: artigos longos que continuam na p√°gina seguinte, tabelas legais

**Crit√©rio de conclus√£o**: VLM extrai hierarquia de p√°ginas legais com >90% accuracy em tipo/n√∫mero e bbox utiliz√°vel para matching.

**Depend√™ncias**: RunPod com A40, vLLM ‚â• 0.11.0, Qwen3-VL-8B-Instruct

---

### FASE 3 ‚Äî Reconciliator (5-7 dias)

**Objetivo**: M√≥dulo central que une VLM + PyMuPDF em chunks com offsets v√°lidos.

**Tarefas**:
1. **Implementar bbox matching**:
   ```python
   def match_bbox_to_text_blocks(
       self,
       vlm_bbox: tuple[float, float, float, float],
       page_blocks: list[TextBlock],
       iou_threshold: float = 0.3,
   ) -> list[TextBlock]:
       """
       Encontra text blocks do PyMuPDF cuja bbox tem IoU (Intersection over Union)
       acima do threshold com a bbox do VLM.

       Retorna blocos ordenados por posi√ß√£o vertical (top‚Üíbottom).
       """
   ```

2. **Implementar constru√ß√£o de offsets**:
   ```python
   def build_offsets(
       self,
       matched_blocks: list[TextBlock],
       canonical_text: str,
   ) -> tuple[int, int]:
       """
       Dado os blocos matchados, determina char_start e char_end no canonical_text.

       Estrat√©gia:
       1. Se blocos s√£o cont√≠guos: start = primeiro.char_start, end = √∫ltimo.char_end
       2. Se h√° gap entre blocos: concatenar texto e usar resolve_child_offsets() como valida√ß√£o
       3. Sempre validar: canonical_text[start:end] cont√©m texto esperado
       """
   ```

3. **Implementar constru√ß√£o de node_id**:
   ```python
   def build_node_id(
       self,
       element: VLMElement,
       document_id: str,
   ) -> str:
       """
       Constr√≥i node_id can√¥nico a partir do output do VLM.

       Exemplos:
       - type="article", number="23" ‚Üí "leis:LEI-14133-2021#ART-023"
       - type="paragraph", number="1", parent="ART-023" ‚Üí "leis:LEI-14133-2021#PAR-023-1"
       - type="inciso", number="V", parent="ART-023" ‚Üí "leis:LEI-14133-2021#INC-023-V"
       """
   ```

4. **Implementar resolu√ß√£o de conflitos**:
   ```python
   def resolve_conflicts(self, chunks: list[ReconciledChunk]) -> list[ReconciledChunk]:
       """
       Resolve conflitos quando:
       - Dois chunks t√™m offsets sobrepostos
       - VLM classifica mesmo trecho de duas formas (baixa confian√ßa)
       - Artigo continua na pr√≥xima p√°gina (cross-page)
       """
   ```

5. **Integrar `resolve_child_offsets()` como fallback/valida√ß√£o**:
   ```python
   def reconcile_element(self, element: VLMElement) -> Optional[ReconciledChunk]:
       # Passo 1: bbox matching
       matched = self.match_bbox_to_text_blocks(element.bbox, page_blocks)

       if matched:
           start, end = self.build_offsets(matched, self.canonical_text)

           # Passo 2: valida√ß√£o via resolve_child_offsets (double-check)
           if element.parent:
               parent_start, parent_end = self.get_parent_range(element.parent)
               try:
                   verified_start, verified_end = resolve_child_offsets(
                       canonical_text=self.canonical_text,
                       parent_start=parent_start,
                       parent_end=parent_end,
                       chunk_text=self.canonical_text[start:end],
                   )
                   return ReconciledChunk(start=verified_start, end=verified_end, ...)
               except OffsetResolutionError:
                   logger.warning(f"Double-check falhou para {element.type} {element.number}")
                   # Usa offset do bbox matching mesmo assim
                   return ReconciledChunk(start=start, end=end, ...)

       # Passo 3: fallback puro via find()
       if element.parent:
           parent_start, parent_end = self.get_parent_range(element.parent)
           start, end = resolve_child_offsets(
               canonical_text=self.canonical_text,
               parent_start=parent_start,
               parent_end=parent_end,
               chunk_text=element.text,
           )
           return ReconciledChunk(start=start, end=end, ...)

       return None  # Falha total ‚Üí alarme
   ```

6. **Testes (n√≠vel industrial)**:
   - Matching: bbox com IoU > 0.3 retorna blocos corretos
   - Offsets: slicing `canonical_text[start:end]` retorna texto esperado para cada tipo de dispositivo
   - Hierarquia: filhos dentro do range do pai (reutilizar l√≥gica dos testes PR13 existentes)
   - Cross-page: artigo que come√ßa na p√°gina 5 e termina na 6
   - Edge cases: artigo com 20+ incisos, al√≠nea dupla, par√°grafo √∫nico
   - Trecho real da Lei 14.133/2021 (reutilizar `LEI_14133_EXCERPT` dos testes existentes)
   - Fallback: quando bbox matching falha, `resolve_child_offsets()` resolve

7. **Gerar offsets.json**:
   - Reconciliator deve produzir offsets.json com objetos por dispositivo contendo: `start`, `end`, `page_number`, `bbox`, `confidence`, `device_type`, `parent_id`
   - Incluir `extraction_method: "pymupdf+qwen3vl"` no header do JSON
   - Validar que todo node_id dos chunks tem entrada correspondente no offsets.json

**Crit√©rio de conclus√£o**: Reconciliator gera chunks com trio Evidence v√°lido (testado pelo PR13 validator) para >95% dos dispositivos de um trecho real da Lei 14.133. offsets.json cont√©m campos extras por dispositivo.

---

### FASE 4 ‚Äî Integra√ß√£o no Pipeline (3-4 dias)

**Objetivo**: Conectar novos componentes ao pipeline de ingest√£o existente, passando pelos gates de seguran√ßa.

**Tarefas**:
1. **Implementar `IntegrityValidator`**:
   - Slicing validation: `canonical_text[start:end]` cont√©m primeiras palavras do chunk
   - Hierarquia: filhos dentro do range do pai
   - Sem sobreposi√ß√£o entre irm√£os
   - Cobertura: artigo cobre todos seus filhos
   - Ordena√ß√£o monot√¥nica de offsets

2. **Criar endpoint de ingest√£o**:
   ```python
   @router.post("/ingest")
   async def ingest_document(pdf_file: UploadFile):
       # 1. PyMuPDF extrai texto + coordenadas
       pymupdf_result = pymupdf_extractor.extract(pdf_path)

       # 2. Renderiza p√°ginas como imagens
       page_images = [pymupdf_extractor.render_page_image(pdf_path, i) for i in range(num_pages)]

       # 3. VLM extrai estrutura hier√°rquica
       vlm_results = vlm_service.extract_document(page_images)

       # 4. Reconciliator une VLM + PyMuPDF
       reconciliator = Reconciliator(pymupdf_result, vlm_results, document_id)
       chunks = reconciliator.reconcile()

       # 5. IntegrityValidator
       integrity_result = integrity_validator.validate(chunks, pymupdf_result.canonical_text)
       if not integrity_result.valid:
           # Alarme WARNING (n√£o bloqueia, mas registra)
           alarm_service.create_alarm(...)

       # 6. canonical_validation (EXISTENTE)
       for chunk in chunks:
           fixed, warnings = validate_and_fix_chunk(chunk.to_dict())

       # 7. PR13 gate (EXISTENTE)
       pr13_result = pr13_validator.validate_chunks(document_id, [c.to_dict() for c in chunks])
       if not pr13_result.valid:
           pr13_validator.create_alarm(db, pr13_result)
           raise HTTPException(422, pr13_result.to_error_response())

       # 8. BGE-M3 embeddings
       embeddings = bge_m3.encode([c.text for c in chunks])

       # 9. Insert Milvus
       milvus_service.insert(collection="leis_v4", chunks=chunks, embeddings=embeddings)

       # 10. Insert Neo4j (hierarquia)
       neo4j_service.insert_hierarchy(chunks)

       return {"status": "ok", "chunks": len(chunks)}
   ```

3. **Atualizar Evidence Link flow** para usar bbox:
   ```python
   # Query-time: snippet_extractor (EXISTENTE) + bbox highlight (NOVO)
   def build_evidence_response(chunk, canonical_text):
       # Snippet via offsets (existente)
       snippet_result = get_snippet_from_chunk(
           canonical_text=canonical_text,
           chunk_text=chunk.text,
           stored_start=chunk.canonical_start,
           stored_end=chunk.canonical_end,
           stored_hash=chunk.canonical_hash,
       )

       # PDF highlight via bbox (NOVO)
       pdf_url = f"{MINIO_URL}/{chunk.document_id}.pdf#page={chunk.page_number}"

       return {
           "snippet": snippet_result.snippet if snippet_result else None,
           "evidence": {
               "document_id": chunk.document_id,
               "page_number": chunk.page_number,
               "bbox": chunk.bbox,           # Para highlight visual no frontend
               "char_start": chunk.canonical_start,
               "char_end": chunk.canonical_end,
               "pdf_url": pdf_url,
           }
       }
   ```

4. **Testes de integra√ß√£o end-to-end**:
   - PDF real da Lei 14.133 ‚Üí pipeline completo ‚Üí chunks no Milvus com trio v√°lido
   - Query ‚Üí busca ‚Üí snippet + bbox ‚Üí evidence link funcional
   - Documento inv√°lido ‚Üí PR13 rejeita ‚Üí alarme no PostgreSQL ‚Üí zero rows no Milvus

5. **Atualizar c√≥digo de upload MinIO** (ref: `docs/ANALISE_MINIO_EVIDENCE_STORAGE.md`):
   - GPU Server (`storage/object_storage.py`): upload do canonical.md (texto PyMuPDF) para bucket `rag-documents`
   - VPS (`evidence/storage_service.py`): upload do canonical.md + offsets.json para bucket `vectorgov-evidence`
   - Ambos os servi√ßos s√£o agn√≥sticos ao formato (aceitam bytes) ‚Äî a mudan√ßa √© no conte√∫do, n√£o no c√≥digo de upload

6. **Validar evidence link completo**:
   - Verificar que `canonical_text[start:end]` retorna snippet correto (sem fallback `find()`)
   - Verificar que `IntegrityValidator` n√£o gera alarmes
   - Verificar que frontend recebe `page_number` e `bbox` para highlight no PDF

**Crit√©rio de conclus√£o**: Pipeline funciona end-to-end com pelo menos 1 documento real. Gates de seguran√ßa rejeitam corretamente chunks inv√°lidos. Evidence links funcionam com snippet + bbox highlight.

---

### FASE 5 ‚Äî Valida√ß√£o e Produ√ß√£o (2-3 dias)

**Objetivo**: Validar qualidade em escala e preparar para produ√ß√£o.

**Tarefas**:
1. **Benchmark de qualidade**:
   - Processar 10+ documentos legais reais (leis, decretos, ac√≥rd√£os)
   - Medir: % de dispositivos corretamente identificados pelo VLM
   - Medir: % de offsets v√°lidos (trio PR13 passa)
   - Medir: % de evidence links funcionais (snippet + bbox)

2. **Stress test no RunPod**:
   - Processar documento de 200+ p√°ginas
   - Medir throughput (p√°ginas/minuto)
   - Monitorar VRAM (Qwen3-VL + BGE-M3 simult√¢neos)
   - Testar recovery de falhas (VLM timeout, OOM)

3. **Monitoramento em produ√ß√£o**:
   - Dashboard de alarmes (alarm_service stats)
   - M√©tricas de confian√ßa do VLM por documento
   - Alertas quando % de chunks com `confidence < 0.8` ultrapassa threshold

**Crit√©rio de conclus√£o**: Pipeline em produ√ß√£o, processando documentos reais, com m√©tricas atingindo targets definidos.

---

## 8. Riscos e Mitiga√ß√µes

| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|--------------|---------|-----------|
| VLM bbox impreciso para texto pequeno (al√≠neas) | M√©dia | Alto | `resolve_child_offsets()` como fallback; threshold de confian√ßa |
| Artigo cross-page (come√ßa p√°g. 5, termina p√°g. 6) | Alta | M√©dio | Reconciliator deve juntar elementos de p√°ginas adjacentes quando VLM detecta continua√ß√£o |
| Tabelas legais (tabela de valores em lei) | M√©dia | M√©dio | Tratar como dispositivo especial; PyMuPDF `get_text("dict")` lida bem com tabelas |
| VRAM insuficiente no A40 para batch grande | Baixa | Alto | `--max_model_len 8096` limita contexto; processar 1 p√°gina por vez se necess√°rio |
| Lat√™ncia alta do VLM (>5s por p√°gina) | M√©dia | Baixo | Ingest√£o √© batch, n√£o real-time; aceit√°vel para processamento de documentos |
| VLM alucina dispositivo inexistente | Baixa | Alto | IntegrityValidator + PR13 gate rejeitam chunks com offsets inv√°lidos |
| Normaliza√ß√£o markdown-specific no IntegrityValidator Layer 6 | M√©dia | M√©dio | Remover na Fase 0: strip `"- "` prefix e regex de espa√ßos em h√≠fens romanos s√£o artefatos do canonical_builder, n√£o existem em texto PyMuPDF |

---

## 9. M√©tricas de Sucesso

| M√©trica | Target | Como medir |
|---------|--------|------------|
| Dispositivos identificados | >95% | Manual review de 5 documentos |
| ADDRESS_MISMATCH rate | <0.5% | Alarmes PR13 / total chunks |
| Evidence link funcional | >98% | snippet_extractor retorna `found=True` |
| Offsets determin√≠sticos | Sim | Hash anti-mismatch est√°vel em re-ingest√£o |
| Throughput | >5 p√°gs/min | Cronometrar ingest√£o de documento |

---

## 10. Testes a Reutilizar

Os testes PR13 existentes s√£o valiosos e devem ser adaptados:

| Arquivo de Teste | Status | Adapta√ß√£o |
|-----------------|--------|-----------|
| `test_pr13_canonical_offsets.py` | ‚úÖ Reutilizar | Trocar MockParsedDocument por MockReconciledChunks |
| `test_pr13_materializer_integration.py` | üîÑ Adaptar | Substituir ChunkMaterializer por Reconciliator nos testes |
| `test_pr13_offset_resolution.py` | ‚úÖ Reutilizar | `resolve_child_offsets()` continua existindo como fallback |
| `test_pr13_span_parser_offsets.py` | üîÑ Adaptar | Trocar SpanParser por VLM+Reconciliator; manter mesmas asser√ß√µes de hierarquia |

**Princ√≠pio**: As ASSER√á√ïES dos testes (filhos dentro do pai, slicing correto, hash determin√≠stico) s√£o independentes do parser. Mudam os fixtures, n√£o as valida√ß√µes.

---

## 11. Ordem de Execu√ß√£o para Claude Code

```
FASE 0: Prepara√ß√£o ‚úÖ CONCLU√çDA (2026-02-06)
‚îú‚îÄ‚îÄ ‚úÖ Criar src/utils/canonical_utils.py (extra√≠do de canonical_offsets.py)
‚îú‚îÄ‚îÄ ‚úÖ Atualizar imports com backward compatibility (re-export + try/except)
‚îú‚îÄ‚îÄ ‚úÖ Criar src/extraction/__init__.py
‚îú‚îÄ‚îÄ ‚úÖ Atualizar ProcessedChunk model (page_number, bbox, confidence)
‚îú‚îÄ‚îÄ ‚úÖ Atualizar schema Milvus leis_v4 (mant√©m nome, adiciona campos VLM)
‚îú‚îÄ‚îÄ ‚úÖ Atualizar dict de inser√ß√£o Milvus em pipeline.py
‚îú‚îÄ‚îÄ ‚è≥ Compatibilidade MinIO: ADIADA (m√≥dulos n√£o existem ainda)
‚îî‚îÄ‚îÄ ‚úÖ Rodar testes existentes ‚Üí 294 passed, 0 failures

FASE 1: PyMuPDF Extractor
‚îú‚îÄ‚îÄ Implementar src/extraction/pymupdf_extractor.py
‚îú‚îÄ‚îÄ Definir formato do canonical.md (texto puro PyMuPDF)
‚îú‚îÄ‚îÄ Testes de determinismo e offsets
‚îî‚îÄ‚îÄ Teste de compara√ß√£o com Docling

FASE 2: VLM Service
‚îú‚îÄ‚îÄ Implementar src/extraction/vlm_service.py
‚îú‚îÄ‚îÄ Desenvolver prompt de extra√ß√£o estruturada
‚îú‚îÄ‚îÄ Testes de accuracy e bbox
‚îî‚îÄ‚îÄ Deploy no RunPod (pode ser em paralelo com Fase 3)

FASE 3: Reconciliator
‚îú‚îÄ‚îÄ Implementar src/extraction/reconciliator.py
‚îú‚îÄ‚îÄ Bbox matching + offset building + node_id construction
‚îú‚îÄ‚îÄ Integrar resolve_child_offsets() como fallback
‚îú‚îÄ‚îÄ Gerar offsets.json com campos extras por dispositivo
‚îú‚îÄ‚îÄ Testes n√≠vel industrial com trecho real Lei 14.133
‚îî‚îÄ‚îÄ Implementar src/extraction/integrity_validator.py

FASE 4: Integra√ß√£o
‚îú‚îÄ‚îÄ Endpoint /ingest
‚îú‚îÄ‚îÄ Conectar ao PR13 + canonical_validation + alarm_service
‚îú‚îÄ‚îÄ Atualizar evidence link flow com bbox
‚îú‚îÄ‚îÄ Atualizar c√≥digo de upload MinIO (GPU Server + VPS)
‚îú‚îÄ‚îÄ Testes end-to-end
‚îî‚îÄ‚îÄ Validar evidence link completo (snippet + bbox highlight)

FASE 5: Valida√ß√£o e Produ√ß√£o
‚îú‚îÄ‚îÄ Benchmark com 10+ documentos
‚îú‚îÄ‚îÄ Stress test RunPod
‚îî‚îÄ‚îÄ Monitoramento
```

---

*Documento gerado para consumo pelo Claude Code na IDE. Cada fase √© independente e test√°vel. Os gates de seguran√ßa (PR13, alarm_service, canonical_validation, snippet_extractor) n√£o s√£o alterados ‚Äî o Reconciliator √© o √∫nico componente novo que produz dados para esses gates.*
